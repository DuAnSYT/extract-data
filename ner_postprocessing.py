#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved NER Postprocessing - Handle fuzzy substring matching
"""

import re
from typing import Dict, List
from collections import Counter
try:
    from rapidfuzz import fuzz
    RAPIDFUZZ_AVAILABLE = True
except ImportError:
    print("Warning: rapidfuzz not installed. Install with: pip install rapidfuzz")
    from difflib import SequenceMatcher
    RAPIDFUZZ_AVAILABLE = False


class NERPostProcessor:
    """Improved Postprocessing cho k·∫øt qu·∫£ NER v·ªõi fuzzy substring matching"""
    
    def __init__(self):
        # Danh s√°ch c√°c prefix h·ªçc h√†m h·ªçc v·ªã c·∫ßn lo·∫°i b·ªè
        self.academic_prefixes = [
            # H·ªçc v·ªã ti·∫øng Vi·ªát
            r'(?i)\b(?:b√°c\s*sƒ©?|bs\.?)\b\s*',
            r'(?i)\b(?:c·ª≠\s*nh√¢n|cn\.?)\b\s*',
            r'(?i)\b(?:th·∫°c\s*sƒ©?|ths\.?)\b\s*',
            r'(?i)\b(?:ti·∫øn\s*sƒ©?|ts\.?)\b\s*',
            r'(?i)\b(?:gi√°o\s*s∆∞|gs\.?)\b\s*',
            r'(?i)\b(?:ph√≥\s*gi√°o\s*s∆∞|pgs\.?)\b\s*',
            
            # Ch·ª©c danh y t·∫ø
            # ===== Chuy√™n khoa (ƒë·ªß c·∫£ d·∫°ng ch·ªØ & vi·∫øt t·∫Øt) =====
            r'(?i)\b(?:chuy√™n\s*khoa\s*(?:i|1)|cki\.?)\b\s*',   # Chuy√™n khoa I / CKI / CK1
            r'(?i)\b(?:chuy√™n\s*khoa\s*(?:ii|2)|ckii\.?)\b\s*', # Chuy√™n khoa II / CKII / CK2

            r'(?i)\bbscc\.?\b',             # BSCC.
            
            # H·ªçc v·ªã ti·∫øng Anh
            r'(?i)\b(?:dr\.?|doctor)\b\s*',
            r'(?i)\b(?:prof\.?|professor)\b\s*',
            r'(?i)\b(?:mr\.?|mrs\.?|ms\.?|miss\.?)\b\s*',
            
            # C√°c ch·ª©c danh kh√°c
            r'(?i)\b(?:d∆∞·ª£c\s*sƒ©?|ds\.?)\b\s*',
            r'(?i)\b(?:y\s*t√°|ƒëi·ªÅu\s*d∆∞·ª°ng)\b\s*',
            r'(?i)\b(?:k·ªπ\s*thu·∫≠t\s*vi√™n|ktv\.?)\b\s*',
            r'(?i)\b(?:th·∫ßy\s*thu·ªëc|tt\.?)\b\s*',
            r'(?i)\b(?:gi√°m\s*ƒë·ªëc|gd\.?|gƒë\.?)\b\s*',
        ]
        
        # Compile regex patterns cho academic prefixes
        self.prefix_patterns = [re.compile(pattern) for pattern in self.academic_prefixes]
        
        # Regex patterns cho c√°c ƒë·ªãa ch·ªâ cho ph√©p
        self.allowed_locations = [
            # TPHCM - c√°c c√°ch vi·∫øt ƒë·∫ßy ƒë·ªß
            r'(?i).*(?:tp\s*\.?\s*)?h[·ªì√¥]\s*ch[√≠i]\s*minh.*',
            r'(?i).*s√†i\s*g[√≤on].*',
            r'(?i).*hcm.*',
            r'(?i).*tphcm.*',
            r'(?i).*th√†nh\s*ph·ªë\s*h·ªì\s*ch√≠\s*minh.*',
            r'(?i).*ho\s*chi\s*minh.*',
            
            # TPHCM - C√°c qu·∫≠n/huy·ªán c·ª• th·ªÉ
            r'(?i).*(?:q\s*\.?\s*|qu·∫≠n\s+)[1-9](?:\d{1,2})?(?:\s|$|,|\.|\-|\/).*',  # Q.1, Q.12, Qu·∫≠n 1, etc.
            r'(?i).*(?:qu·∫≠n|q\s*\.?\s*)(?:b√¨nh\s*th·∫°nh|t√¢n\s*b√¨nh|g√≤\s*v·∫•p|ph√∫\s*nhu·∫≠n|t√¢n\s*ph√∫|b√¨nh\s*t√¢n).*',
            r'(?i).*(?:qu·∫≠n|q\s*\.?\s*)(?:th·ªß\s*ƒë·ª©c).*',
            r'(?i).*th·ªß\s*ƒë·ª©c.*',
            r'(?i).*(?:huy·ªán|h\s*\.?\s*)(?:c·ªß\s*chi|h√≥c\s*m√¥n|b√¨nh\s*ch√°nh|nh√†\s*b√®|c·∫ßn\s*gi·ªù).*',
            
            # B√¨nh D∆∞∆°ng
            r'(?i).*b[√¨i]nh\s*d[∆∞∆°uong].*',
            r'(?i).*binh\s*duong.*',
            r'(?i).*(?:tp\s*\.?\s*|th√†nh\s*ph·ªë\s+)?th·ªß\s*d·∫ßu\s*m·ªôt.*',
            r'(?i).*(?:tp\s*\.?\s*|th√†nh\s*ph·ªë\s+)?dƒ©\s*an.*',
            r'(?i).*(?:tp\s*\.?\s*|th√†nh\s*ph·ªë\s+)?thu·∫≠n\s*an.*',
            r'(?i).*(?:th·ªã\s*x√£\s+|tx\s*\.?\s*)?t√¢n\s*uy√™n.*',
            r'(?i).*(?:th·ªã\s*x√£\s+|tx\s*\.?\s*)?b·∫øn\s*c√°t.*',
            r'(?i).*(?:huy·ªán|h\s*\.?\s*)(?:b√†u\s*b√†ng|d·∫ßu\s*ti·∫øng|ph√∫\s*gi√°o|b·∫Øc\s*t√¢n\s*uy√™n).*',
            
            # B√† R·ªãa - V≈©ng T√†u
            r'(?i).*b[√†a]\s*r[·ªãi]a\s*v[≈©u]ng\s*t[√†a]u.*',
            r'(?i).*ba\s*ria\s*vung\s*tau.*',
            r'(?i).*brvt.*',
            r'(?i).*(?:tp\s*\.?\s*|th√†nh\s*ph·ªë\s+)?v≈©ng\s*t√†u.*',
            r'(?i).*(?:tp\s*\.?\s*|th√†nh\s*ph·ªë\s+)?b√†\s*r·ªãa.*',
            r'(?i).*(?:th·ªã\s*x√£\s+|tx\s*\.?\s*)?ph√∫\s*m·ªπ.*',
            r'(?i).*(?:huy·ªán|h\s*\.?\s*)(?:ch√¢u\s*ƒë·ª©c|xuy√™n\s*m·ªôc|ƒë·∫•t\s*ƒë·ªè|t√¢n\s*th√†nh|long\s*ƒëi·ªÅn).*',
            r'(?i).*c√¥n\s*ƒë·∫£o.*',
        ]
        
        # Compile regex patterns
        self.location_patterns = [re.compile(pattern) for pattern in self.allowed_locations]
    
    def fuzzy_similarity(self, a: str, b: str) -> float:
        """T√≠nh ƒë·ªô t∆∞∆°ng t·ª± fuzzy gi·ªØa 2 string v·ªõi RapidFuzz"""
        if RAPIDFUZZ_AVAILABLE:
            return fuzz.ratio(a.lower().strip(), b.lower().strip()) / 100.0
        else:
            # Fallback to difflib
            return SequenceMatcher(None, a.lower().strip(), b.lower().strip()).ratio()
    
    def fuzzy_partial_similarity(self, a: str, b: str) -> float:
        """T√≠nh ƒë·ªô t∆∞∆°ng t·ª± partial (substring) fuzzy"""
        if RAPIDFUZZ_AVAILABLE:
            return fuzz.partial_ratio(a.lower().strip(), b.lower().strip()) / 100.0
        else:
            # Fallback: t·ª± implement partial ratio ƒë∆°n gi·∫£n
            a_clean = a.lower().strip()
            b_clean = b.lower().strip()
            
            if len(a_clean) > len(b_clean):
                longer, shorter = a_clean, b_clean
            else:
                longer, shorter = b_clean, a_clean
            
            best_ratio = 0
            for i in range(len(longer) - len(shorter) + 1):
                substring = longer[i:i + len(shorter)]
                ratio = SequenceMatcher(None, shorter, substring).ratio()
                best_ratio = max(best_ratio, ratio)
            
            return best_ratio
    
    def is_substring_or_similar(self, short: str, long: str, threshold: float = 0.8, partial_threshold: float = 0.9) -> bool:
        """
        Ki·ªÉm tra xem short c√≥ ph·∫£i substring ho·∫∑c t∆∞∆°ng t·ª± v·ªõi long kh√¥ng
        Bao g·ªìm c·∫£ fuzzy substring matching
        
        Args:
            short: Chu·ªói ng·∫Øn h∆°n
            long: Chu·ªói d√†i h∆°n  
            threshold: Ng∆∞·ª°ng cho full similarity
            partial_threshold: Ng∆∞·ª°ng cho partial similarity (cao h∆°n v√¨ ch·ªâ so s√°nh 1 ph·∫ßn)
        """
        short_clean = short.lower().strip()
        long_clean = long.lower().strip()
        
        # Case 1: Exact substring
        if short_clean in long_clean:
            return True
        
        # Case 2: Full fuzzy similarity (to√†n b·ªô string t∆∞∆°ng t·ª±)
        if self.fuzzy_similarity(short_clean, long_clean) >= threshold:
            return True
        
        # Case 3: Partial fuzzy similarity (substring nh∆∞ng vi·∫øt kh√°c 1 t√≠)
        # V√≠ d·ª•: "HANA" trong "H·ªá th·ªëng th·∫©m m·ªπ HANNA"
        if self.fuzzy_partial_similarity(short_clean, long_clean) >= partial_threshold:
            return True
            
        return False
    
    def is_similar_organizations(self, org1: str, org2: str, threshold: float = 0.8, partial_threshold: float = 0.9) -> bool:
        """
        Ki·ªÉm tra 2 organizations c√≥ t∆∞∆°ng t·ª± nhau kh√¥ng
        X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p substring v√† fuzzy matching
        """
        org1_clean = org1.lower().strip()
        org2_clean = org2.lower().strip()
        
        # X√°c ƒë·ªãnh c√°i n√†o ng·∫Øn h∆°n, d√†i h∆°n
        if len(org1_clean) <= len(org2_clean):
            shorter, longer = org1_clean, org2_clean
        else:
            shorter, longer = org2_clean, org1_clean
        
        # Ki·ªÉm tra substring ho·∫∑c similar
        return self.is_substring_or_similar(shorter, longer, threshold, partial_threshold)
    
    def is_allowed_location(self, location: str) -> bool:
        """Ki·ªÉm tra xem ƒë·ªãa ch·ªâ c√≥ thu·ªôc v√πng cho ph√©p kh√¥ng"""
        location_clean = location.strip()
        
        for pattern in self.location_patterns:
            if pattern.search(location_clean):
                return True
        
        return False
    
    def remove_academic_prefix(self, person_name: str) -> str:
        """
        Lo·∫°i b·ªè c√°c prefix h·ªçc h√†m h·ªçc v·ªã t·ª´ t√™n ng∆∞·ªùi
        
        Args:
            person_name: T√™n ng∆∞·ªùi c√≥ th·ªÉ ch·ª©a prefix
            
        Returns:
            T√™n ng∆∞·ªùi ƒë√£ lo·∫°i b·ªè prefix
        """
        cleaned_name = person_name.strip()
        
        # √Åp d·ª•ng t·ª´ng pattern ƒë·ªÉ lo·∫°i b·ªè prefix
        for pattern in self.prefix_patterns:
            cleaned_name = pattern.sub('', cleaned_name).strip()
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a v√† normalize
        cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()
        
        # N·∫øu sau khi lo·∫°i b·ªè prefix m√† t√™n tr·ªëng th√¨ tr·∫£ v·ªÅ t√™n g·ªëc
        if not cleaned_name:
            return person_name.strip()
        
        return cleaned_name
    
    def expand_medical_abbreviations(self, org_name: str) -> str:
        """
        M·ªü r·ªông c√°c t·ª´ vi·∫øt t·∫Øt trong t√™n c∆° s·ªü y t·∫ø
        
        Args:
            org_name: T√™n c∆° s·ªü c√≥ th·ªÉ ch·ª©a vi·∫øt t·∫Øt
            
        Returns:
            T√™n c∆° s·ªü ƒë√£ m·ªü r·ªông vi·∫øt t·∫Øt
        """
        if not org_name:
            return org_name
            
        # Danh s√°ch c√°c vi·∫øt t·∫Øt y t·∫ø ph·ªï bi·∫øn
        abbreviations = {
            # ===== Vi·∫øt t·∫Øt b·ªánh vi·ªán & ƒëa khoa =====
            r'\bbvdk\b': 'b·ªánh vi·ªán ƒëa khoa',  # ƒë·∫∑t tr∆∞·ªõc ƒë·ªÉ tr√°nh xung ƒë·ªôt v·ªõi bv + dk
            r'\bbvƒëk\b': 'b·ªánh vi·ªán ƒëa khoa',   # th√™m d·∫°ng c√≥ ƒë
            r'\bbv\b': 'b·ªánh vi·ªán',
            r'\bdk\b': 'ƒëa khoa',
            r'\bƒëk\b': 'ƒëa khoa',
            r'\bttyt\b': 'trung t√¢m y t·∫ø',

            # ===== C√°c vi·∫øt t·∫Øt chuy√™n khoa =====
            r'\bpk\b': 'ph√≤ng kh√°m',
            r'\btm\b': 'th·∫©m m·ªπ',
            r'\btw\b': 'trung ∆∞∆°ng',
            r'\bqt\b': 'qu·ªëc t·∫ø',
            r'\bƒëhyd?\b': 'ƒë·∫°i h·ªçc y d∆∞·ª£c',
            r'\bdhyd?\b': 'ƒë·∫°i h·ªçc y d∆∞·ª£c',
            r'\btkb\b': 'tai - m≈©i - h·ªçng',
            r'\btmh\b': 'tai - m≈©i - h·ªçng',
            r'\brhm?\b': 'rƒÉng - h√†m - m·∫∑t',
            r'\bphcn\b': 'ph·ª•c h·ªìi ch·ª©c nƒÉng',
            r'\bcchs\b': 'c·∫•p c·ª©u h·ªìi s·ª©c',

            # ===== Vi·∫øt t·∫Øt ƒë·ªãa danh =====
            r'\btp\.?\s*(?:h·ªì\s*ch√≠\s*minh|hcm)\b': 'th√†nh ph·ªë h·ªì ch√≠ minh',
            r'\btp\.?(?=\s|$)': 'th√†nh ph·ªë',
            r'\bhcm\b': 'h·ªì ch√≠ minh',
            r'\bq\.?\s*(\d+)\b': r'qu·∫≠n \1',

            #==== C√°c vi·∫øt t·∫Øt kh√°c =====
            r'\bCP\b': 'c·ªï ph·∫ßn',
            r'\bcty\b': 'c√¥ng ty',
            r'\btnhh\b': 'tr√°ch nhi·ªám h·ªØu h·∫°n',
        }
        
        expanded = org_name.lower().strip()
        
        # √Åp d·ª•ng t·ª´ng pattern ƒë·ªÉ m·ªü r·ªông vi·∫øt t·∫Øt
        for abbrev_pattern, full_form in abbreviations.items():
            expanded = re.sub(abbrev_pattern, full_form, expanded, flags=re.IGNORECASE)
        
        # Normalize kho·∫£ng tr·∫Øng
        expanded = re.sub(r'\s+', ' ', expanded).strip()
        
        # Chuy·ªÉn v·ªÅ title case
        if expanded:
            words = expanded.split()
            capitalized_words = []
            for word in words:
                if word:
                    capitalized_words.append(word.capitalize())
            expanded = ' '.join(capitalized_words)
        
        return expanded if expanded else org_name
    
    def process_organizations(self, org_entities: List[Dict], threshold: float = 0.8, partial_threshold: float = 0.9) -> str:
        """
        X·ª≠ l√Ω danh s√°ch organizations v·ªõi improved fuzzy matching
        Tr·∫£ v·ªÅ list organization name
        
        Args:
            org_entities: List entities c·ªßa ORG
            threshold: Ng∆∞·ª°ng cho full similarity  
            partial_threshold: Ng∆∞·ª°ng cho partial similarity
        """
        if not org_entities:
            return []
        
        # L·∫•y t·∫•t c·∫£ text c·ªßa organizations v√† m·ªü r·ªông vi·∫øt t·∫Øt
        org_texts = [self.expand_medical_abbreviations(entity['text'].strip()) for entity in org_entities]
        
        if len(org_texts) == 1:
            return org_texts
        
        # ƒê·∫øm frequency c·ªßa t·ª´ng organization (exact match)
        org_counter = Counter(org_texts)
        
        # Group c√°c organizations t∆∞∆°ng t·ª± nhau
        grouped_orgs = {}
        processed = set()
        
        for i, org in enumerate(org_texts):
            if org in processed:
                continue
                
            # T√¨m t·∫•t c·∫£ organizations t∆∞∆°ng t·ª± v·ªõi org n√†y
            similar_orgs = [org]
            
            for j, other_org in enumerate(org_texts):
                if i != j and other_org not in processed:
                    # Ki·ªÉm tra xem c√≥ t∆∞∆°ng t·ª± nhau kh√¥ng (bao g·ªìm fuzzy substring)
                    if self.is_similar_organizations(org, other_org, threshold, partial_threshold):
                        similar_orgs.append(other_org)
            
            # Ch·ªçn c√°i d√†i nh·∫•t trong group
            longest_org = max(similar_orgs, key=len)
            
            # T√≠nh t·ªïng count cho group n√†y
            total_count = sum(org_counter[similar_org] for similar_org in similar_orgs)
            
            grouped_orgs[longest_org] = total_count
            
            # ƒê√°nh d·∫•u ƒë√£ x·ª≠ l√Ω
            for similar_org in similar_orgs:
                processed.add(similar_org)
        
        # # Ch·ªçn organization c√≥ count cao nh·∫•t
        # if grouped_orgs:
        #     best_org = max(grouped_orgs.items(), key=lambda x: x[1])[0]
        #     return best_org.strip()

        if grouped_orgs:
            sorted_orgs = sorted(grouped_orgs.items(), key=lambda x: x[1], reverse=True)
            return [org for org, count in sorted_orgs]

        return []
    
    def process_addresses(self, loc_entities: List[Dict]) -> List[str]:
        """
        X·ª≠ l√Ω danh s√°ch ƒë·ªãa ch·ªâ, ch·ªâ gi·ªØ l·∫°i nh·ªØng ƒë·ªãa ch·ªâ thu·ªôc v√πng cho ph√©p v√† d√†i h∆°n 10 k√Ω t·ª±
        """
        if not loc_entities:
            return []
        
        allowed_addresses = []
        seen_addresses = set()
        
        for entity in loc_entities:
            address = entity['text'].strip()
            address_lower = address.lower()
            
            # Ki·ªÉm tra tr√πng l·∫∑p, location h·ª£p l·ªá v√† ƒë·ªô d√†i >= 10 k√Ω t·ª±
            if (address_lower not in seen_addresses and 
                self.is_allowed_location(address) and 
                len(address) >= 10):
                allowed_addresses.append(address)
                seen_addresses.add(address_lower)
        
        return allowed_addresses
    
    def clean_person_name(self, name: str) -> str:
        """
        L√†m s·∫°ch t√™n ng∆∞·ªùi - lo·∫°i b·ªè k√Ω t·ª± d∆∞ th·ª´a, prefix, etc.
        """
        if not name:
            return ""
            
        cleaned = name.strip()
        
        # Lo·∫°i b·ªè prefix h·ªçc h√†m h·ªçc v·ªã
        cleaned = self.remove_academic_prefix(cleaned)
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± d∆∞ th·ª´a ·ªü ƒë·∫ßu v√† cu·ªëi
        cleaned = re.sub(r'^[-.\s]+', '', cleaned)  # Lo·∫°i b·ªè -, . ·ªü ƒë·∫ßu
        cleaned = re.sub(r'[-.\s]+$', '', cleaned)  # Lo·∫°i b·ªè -, . ·ªü cu·ªëi
        
        # Normalize kho·∫£ng tr·∫Øng
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # Normalize case - chuy·ªÉn v·ªÅ title case cho t√™n ng∆∞·ªùi Vi·ªát
        if cleaned and re.search(r'[a-zA-Z√Ä-·ªπ]', cleaned):
            cleaned = self.normalize_vietnamese_name(cleaned)
        
        return cleaned
    
    def normalize_vietnamese_name(self, name: str) -> str:
        """
        Chu·∫©n h√≥a t√™n ng∆∞·ªùi Vi·ªát v·ªÅ d·∫°ng title case
        """
        # Split th√†nh c√°c t·ª´ v√† capitalize t·ª´ng t·ª´
        words = name.split()
        normalized_words = []
        
        for word in words:
            # Capitalize t·ª´ng t·ª´ (ch·ªØ ƒë·∫ßu vi·∫øt hoa, c√≤n l·∫°i vi·∫øt th∆∞·ªùng)
            if word:
                normalized_words.append(word.capitalize())
        
        return ' '.join(normalized_words)
    
    def is_substring_name(self, short_name: str, long_name: str) -> bool:
        """
        Ki·ªÉm tra xem short_name c√≥ ph·∫£i l√† substring c·ªßa long_name kh√¥ng
        (ƒë·ªÉ dedup c√°c tr∆∞·ªùng h·ª£p nh∆∞ "V≈©" trong "H·ªì Cao V≈©")
        """
        if not short_name or not long_name:
            return False
            
        short_clean = short_name.lower().strip()
        long_clean = long_name.lower().strip()
        
        # Ki·ªÉm tra substring exact
        if short_clean in long_clean and len(short_clean) < len(long_clean):
            return True
            
        return False
    
    def process_persons(self, per_entities: List[Dict]) -> List[str]:
        """
        X·ª≠ l√Ω danh s√°ch persons v·ªõi advanced cleaning v√† case-insensitive substring dedup
        """
        if not per_entities:
            return []
        
        # B∆∞·ªõc 1: Clean t·∫•t c·∫£ t√™n
        cleaned_persons = []
        for entity in per_entities:
            person_raw = entity['text'].strip()
            person_cleaned = self.clean_person_name(person_raw)
            
            # Ch·ªâ gi·ªØ t√™n c√≥ √Ω nghƒ©a (>= 2 k√Ω t·ª±, kh√¥ng ch·ªâ to√†n k√Ω t·ª± ƒë·∫∑c bi·ªát)
            if person_cleaned and len(person_cleaned) >= 2 and re.search(r'[a-zA-Z√Ä-·ªπ]', person_cleaned):
                cleaned_persons.append(person_cleaned)
        
        if not cleaned_persons:
            return []
        
        # B∆∞·ªõc 2: Lo·∫°i b·ªè exact duplicates (case-insensitive)
        seen_lower = {}
        unique_persons = []
        
        for person in cleaned_persons:
            person_lower = person.lower()
            if person_lower not in seen_lower:
                seen_lower[person_lower] = person
                unique_persons.append(person)
        
        # B∆∞·ªõc 3: Lo·∫°i b·ªè substring duplicates (case-insensitive, gi·ªØ chu·ªói d√†i nh·∫•t)
        final_persons = []
        
        for person in unique_persons:
            is_substring = False
            
            # Ki·ªÉm tra xem person n√†y c√≥ ph·∫£i substring c·ªßa ai ƒë√£ c√≥ trong final_persons kh√¥ng
            for existing in final_persons:
                if self.is_substring_name(person, existing):
                    is_substring = True
                    break
            
            if not is_substring:
                # Ki·ªÉm tra ng∆∞·ª£c l·∫°i: person n√†y c√≥ ch·ª©a substring n√†o trong final_persons kh√¥ng
                # N·∫øu c√≥ th√¨ thay th·∫ø (gi·ªØ c√°i d√†i h∆°n)
                to_remove = []
                for i, existing in enumerate(final_persons):
                    if self.is_substring_name(existing, person):
                        to_remove.append(i)
                
                # X√≥a c√°c substring c≈©
                for i in reversed(to_remove):
                    final_persons.pop(i)
                
                # Th√™m person hi·ªán t·∫°i
                final_persons.append(person)
        
        return final_persons
    
    def postprocess(self, ner_results: Dict, threshold: float = 0.8, partial_threshold: float = 0.9) -> Dict:
        """
        Main postprocessing function v·ªõi improved fuzzy matching
        
        Args:
            ner_results: Output t·ª´ VietnameseNERPredictor
            threshold: Ng∆∞·ª°ng cho full similarity (0.0-1.0)
            partial_threshold: Ng∆∞·ª°ng cho partial similarity (0.0-1.0)
            
        Returns:
            Dict v·ªõi format: {
                'organization_name': List[str],
                'address': List[str],
                'person': List[str]
            }
        """
        entities_by_type = ner_results.get('entities_by_type', {})
        
        # X·ª≠ l√Ω organizations v·ªõi improved logic
        org_entities = entities_by_type.get('ORG', [])
        organization_names = self.process_organizations(org_entities, threshold, partial_threshold)
        
        # X·ª≠ l√Ω locations/addresses
        loc_entities = entities_by_type.get('ADDR', [])
        addresses = self.process_addresses(loc_entities)
        
        # X·ª≠ l√Ω persons
        per_entities = entities_by_type.get('PER', [])
        persons = self.process_persons(per_entities)
        
        # C√≥ th·ªÉ th√™m x·ª≠ l√Ω cho c√°c entity type kh√°c n·∫øu c·∫ßn
        gpe_entities = entities_by_type.get('GPE', [])
        if gpe_entities:
            gpe_addresses = self.process_addresses(gpe_entities)
            # Merge v√† lo·∫°i b·ªè tr√πng l·∫∑p
            all_addresses = addresses + gpe_addresses
            addresses = list(dict.fromkeys(all_addresses))
        
        return {
            'organization_names': organization_names,
            'addresses': addresses,
            'persons': persons
        }


# üöÄ QUICK FUNCTION - Global instance
_processor = None

def get_processor():
    """Lazy initialization c·ªßa processor"""
    global _processor
    if _processor is None:
        _processor = NERPostProcessor()
    return _processor


def quick_postprocess(ner_results: Dict, threshold: float = 0.8, partial_threshold: float = 0.9) -> Dict:
    """
    üöÄ QUICK FUNCTION: Improved postprocess v·ªõi fuzzy substring matching
    
    Args:
        ner_results: Output t·ª´ VietnameseNERPredictor
        threshold: Ng∆∞·ª°ng cho full similarity (0.0-1.0)
        partial_threshold: Ng∆∞·ª°ng cho partial similarity (0.0-1.0, th∆∞·ªùng cao h∆°n threshold)
        
    Returns:
        Dict v·ªõi format: {
            'organization_name': str,
            'address': List[str],
            'person': List[str]
        }
    """
    processor = get_processor()
    return processor.postprocess(ner_results, threshold, partial_threshold)


def test_improved_fuzzy():
    """üß™ Test improved fuzzy substring matching v√† academic prefix removal"""
    
    processor = NERPostProcessor()
    
    # Test cases cho medical abbreviation expansion
    print("=== Test Medical Abbreviation Expansion ===")
    test_orgs = [
        "BV Ch·ª£ R·∫´y",
        "BVDK T√¢n T·∫°o", 
        "BV TM Hana",
        "Ph√≤ng kh√°m PK ABC",
        "BV NT Q7",
        "DHYD TPHCM",
        "BV RƒÉng H√†m M·∫∑t RHMM",
        "Ph√≤ng kh√°m TMH Q1",
        "BV Tim TPHCM",
        "Trung t√¢m PHCN",
        "BV CCHS 115",
        "B·ªánh vi·ªán T·ª´ D≈©",  # Kh√¥ng c√≥ vi·∫øt t·∫Øt
        "BV Q.1",
        "BVDK Q 10"
    ]
    
    for org in test_orgs:
        expanded = processor.expand_medical_abbreviations(org)
        print(f"'{org}' ‚Üí '{expanded}'")
    print()
    
    # Test cases cho address cleaning
    print("=== Test Address Cleaning ===")
    test_addresses = [
        "123 Nguy·ªÖn VƒÉn A, Qu·∫≠n 1, TP.HCM (g·∫ßn ch·ª£ B·∫øn Th√†nh)",
        "456 L√™ L·ª£i, Q.3, TPHCM [t·∫ßng 2, to√† nh√† ABC]",
        "789 Tr·∫ßn H∆∞ng ƒê·∫°o, Q1 {c·∫°nh ng√¢n h√†ng Vietcombank}",
        "321 Hai B√† Tr∆∞ng, Qu·∫≠n 3, g·∫ßn si√™u th·ªã BigC",
        "654 V√µ VƒÉn T·∫ßn, Q.3, ƒë·ªëi di·ªán b·ªánh vi·ªán Ch·ª£ R·∫´y",
        "987 Nam K·ª≥ Kh·ªüi Nghƒ©a, ph√≠a sau ch·ª£ Nguy·ªÖn Thi·ªán Thu·∫≠t",
        "159 Pasteur, Qu·∫≠n 1, l·∫ßu 3 block A",
        "753 C√°ch M·∫°ng Th√°ng 8, t·∫ßng 5 to√† nh√† Diamond Plaza",
        "852 Nguy·ªÖn Th·ªã Minh Khai, to√† nh√† Landmark 81",
        "951 ƒê∆∞·ªùng D1, Qu·∫≠n 7, s·ªë 25A cƒÉn h·ªô B1-08",
        "- 147 Nguy·ªÖn Du, Q.1, TPHCM.,",  # C√≥ k√Ω t·ª± d∆∞ th·ª´a
        ". 258 L√Ω T·ª± Tr·ªçng, Q.1 ;;",      # C√≥ k√Ω t·ª± d∆∞ th·ª´a
        "123 ABC ()",                      # Ngo·∫∑c r·ªóng
        "456 XYZ []",                      # Ngo·∫∑c vu√¥ng r·ªóng
        "789 DEF {}",                      # Ngo·∫∑c nh·ªçn r·ªóng
        "B·ªánh vi·ªán Ch·ª£ R·∫´y, 201B Nguy·ªÖn Ch√≠ Thanh, Qu·∫≠n 5, TP.HCM",  # ƒê·ªãa ch·ªâ b√¨nh th∆∞·ªùng
    ]
    
    for address in test_addresses:
        cleaned = processor.clean_address(address)
        print(f"'{address}'")
        print(f"‚Üí '{cleaned}'")
        print()
    
    # Test cases cho person name cleaning v√† academic prefix removal
    print("=== Test Person Name Cleaning ===")
    test_persons = [
        "B√°c sƒ© Nguy·ªÖn VƒÉn A",
        "BS. Tr·∫ßn Th·ªã B", 
        "Th·∫°c sƒ© L√™ Minh C",
        "TS. Ph·∫°m ƒê·ª©c D",
        "Gi√°o s∆∞ Ho√†ng Th·ªã E",
        "PGS.TS. V≈© VƒÉn F",
        "Dr. John Smith",
        "CKI Nguy·ªÖn Th√†nh G",
        "CKI. L√™ VƒÉn M", 
        "CKII L√Ω Th·ªã H",
        "CKII. Ph·∫°m Th·ªã N",
        "CK1. Tr·∫ßn Minh O",
        "CK2. Ho√†ng Th·ªã P",
        "D∆∞·ª£c sƒ© Cao Minh I",
        "Y t√° ƒêinh Th·ªã J",
        "Mr. David Wilson",
        "Nguy·ªÖn VƒÉn K",  # Kh√¥ng c√≥ prefix
        "B√°c sƒ©",        # Ch·ªâ c√≥ prefix
        "- H·ªì Cao V≈©",   # C√≥ d·∫•u - ·ªü ƒë·∫ßu
        ". H·ªì Cao V≈©",   # C√≥ d·∫•u . ·ªü ƒë·∫ßu
        ". V≈©",          # Ch·ªâ c√≥ t√™n ng·∫Øn
        "H·ªì Cao V≈©",     # T√™n b√¨nh th∆∞·ªùng
    ]
    
    for person in test_persons:
        cleaned = processor.clean_person_name(person)
        print(f"'{person}' ‚Üí '{cleaned}'")
    print()
    
    # Test substring deduplication
    print("=== Test Substring Deduplication ===")
    test_entities = [
        {'text': '- H·ªì Cao V≈©', 'label': 'PER', 'confidence': 0.95},
        {'text': '. H·ªì Cao V≈©', 'label': 'PER', 'confidence': 0.92},
        {'text': '. V≈©', 'label': 'PER', 'confidence': 0.88},
        {'text': 'H·ªì Cao V≈©', 'label': 'PER', 'confidence': 0.90},
        {'text': 'B√°c sƒ© Nguy·ªÖn VƒÉn A', 'label': 'PER', 'confidence': 0.93},
        {'text': 'Nguy·ªÖn VƒÉn A', 'label': 'PER', 'confidence': 0.89},
        {'text': 'A', 'label': 'PER', 'confidence': 0.85},
    ]
    
    result_persons = processor.process_persons(test_entities)
    print("Input entities:")
    for entity in test_entities:
        print(f"  - '{entity['text']}'")
    print(f"After processing: {result_persons}")
    print()
    
    # Test case-insensitive deduplication (t·ª´ data th·ª±c c·ªßa user)
    print("=== Test Case-Insensitive Deduplication (Real Data) ===")
    real_test_entities = [
        {'text': 'B√ÅC VƒÇN', 'label': 'PER', 'confidence': 0.95},
        {'text': 'b√°c VƒÉn', 'label': 'PER', 'confidence': 0.92},
        {'text': 'tr∆∞·ªüng khoa L√™ Vi·∫øt VƒÉn', 'label': 'PER', 'confidence': 0.90},
    ]
    
    real_result_persons = processor.process_persons(real_test_entities)
    print("Input entities:")
    for entity in real_test_entities:
        print(f"  - '{entity['text']}'")
    print(f"After processing: {real_result_persons}")
    print()
    
    # Test cases cho fuzzy substring
    test_cases = [
        ("HANA", "H·ªá th·ªëng th·∫©m m·ªπ HANNA"),  # Substring nh∆∞ng vi·∫øt kh√°c 1 t√≠
        ("VinMart", "Si√™u th·ªã VinMart Plus"), # Substring exact
        ("BigC", "BigC ThƒÉng Long"),          # Substring exact
        ("Lotte", "Lotte Cinema"),            # Substring exact
        ("ABC", "XYZ Company"),               # Kh√¥ng li√™n quan
        ("Vincom", "Vincom Center"),          # Substring exact
        ("TGDD", "Th·∫ø Gi·ªõi Di ƒê·ªông TGDD"),   # Substring exact
    ]
    
    print("=== Test Fuzzy Substring Matching ===")
    for short, long in test_cases:
        is_similar = processor.is_similar_organizations(short, long)
        partial_sim = processor.fuzzy_partial_similarity(short, long)
        full_sim = processor.fuzzy_similarity(short, long)
        
        print(f"'{short}' vs '{long}':")
        print(f"  Similar: {is_similar}")
        print(f"  Partial similarity: {partial_sim:.3f}")
        print(f"  Full similarity: {full_sim:.3f}")
        print()
    
    # Test address processing v·ªõi real data
    print("=== Test Address Processing (Real Data) ===")
    test_address_entities = [
        {'text': '123 Nguy·ªÖn VƒÉn A, Qu·∫≠n 1, TP.HCM (g·∫ßn ch·ª£ B·∫øn Th√†nh)', 'label': 'ADDR', 'confidence': 0.95},
        {'text': '456 L√™ L·ª£i, Q.3, TPHCM [t·∫ßng 2]', 'label': 'ADDR', 'confidence': 0.92},
        {'text': 'B·ªánh vi·ªán Ch·ª£ R·∫´y, 201B Nguy·ªÖn Ch√≠ Thanh, Qu·∫≠n 5, TP.HCM', 'label': 'ADDR', 'confidence': 0.90},
        {'text': 'H√† N·ªôi', 'label': 'ADDR', 'confidence': 0.85},  # Kh√¥ng thu·ªôc v√πng cho ph√©p
        {'text': '789 ABC, g·∫ßn si√™u th·ªã', 'label': 'ADDR', 'confidence': 0.88},  # Qu√° ng·∫Øn sau khi clean
    ]
    
    result_addresses = processor.process_addresses(test_address_entities)
    print("Input address entities:")
    for entity in test_address_entities:
        print(f"  - '{entity['text']}'")
    print(f"After processing: {result_addresses}")
    print()
    
    # Test v·ªõi real data
    test_ner_results = {
        'entities_by_type': {
            'ORG': [
                {'text': 'HANA', 'label': 'ORG', 'confidence': 0.95},
                {'text': 'H·ªá th·ªëng th·∫©m m·ªπ HANNA', 'label': 'ORG', 'confidence': 0.92},
                {'text': 'HANA', 'label': 'ORG', 'confidence': 0.93},
                {'text': 'VinMart', 'label': 'ORG', 'confidence': 0.90},
                {'text': 'Si√™u th·ªã VinMart Plus', 'label': 'ORG', 'confidence': 0.88},
            ],
            'ADDR': [
                {'text': '123 Nguy·ªÖn VƒÉn A, Qu·∫≠n 1, TP.HCM (g·∫ßn ch·ª£)', 'label': 'ADDR', 'confidence': 0.89},
                {'text': 'Th·ªß ƒê·ª©c, TP.HCM [khu v·ª±c trung t√¢m]', 'label': 'ADDR', 'confidence': 0.87},
                {'text': 'H√† N·ªôi', 'label': 'ADDR', 'confidence': 0.85}
            ],
            'PER': [
                {'text': 'B√°c sƒ© Nguy·ªÖn VƒÉn A', 'label': 'PER', 'confidence': 0.92},
                {'text': 'TS. Tr·∫ßn Th·ªã B', 'label': 'PER', 'confidence': 0.88},
                {'text': 'Nguy·ªÖn VƒÉn A', 'label': 'PER', 'confidence': 0.90},
                {'text': 'CKI L√™ Minh C', 'label': 'PER', 'confidence': 0.85},
                {'text': 'Dr. John Smith', 'label': 'PER', 'confidence': 0.87}
            ]
        }
    }
    
    print("=== Test Real NER Data ===")
    result = quick_postprocess(test_ner_results)
    print(f"Organizations: {result['organization_names']}")
    print(f"Addresses: {result['addresses']}")
    print(f"Persons: {result['persons']}")
    
    print(f"\n=== Using RapidFuzz: {RAPIDFUZZ_AVAILABLE} ===")


if __name__ == "__main__":
    test_improved_fuzzy()