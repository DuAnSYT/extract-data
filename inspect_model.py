import gradio as gr
import torch
import os
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification
from typing import List, Dict
from underthesea import sent_tokenize
import numpy as np

# ==========================================
# PH·∫¶N 1: CLASS INFERENCE C·ª¶A B·∫†N (GI·ªÆ NGUY√äN LOGIC)
# ==========================================

class VietnameseNERPredictor:
    """Simple Vietnamese NER Predictor"""
    
    def __init__(self, model_name_or_path: str):
        """Initialize predictor with model"""
        print(f"Loading model from: {model_name_or_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
        self.model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)
        self.model.eval()

        self._TOKENIZER = self.tokenizer
        self.MAX_LENGTH = 512
        self.tokenizer.model_max_length = self.MAX_LENGTH
        
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print("‚úÖ Model loaded on GPU")
        else:
            print("‚ö†Ô∏è Model loaded on CPU")

    def fix_bio_tags(self, tags):
        """Fix invalid BIO tag sequences"""
        fixed_tags = list(tags)
        for i in range(len(fixed_tags)):
            if fixed_tags[i].startswith('I-'):
                entity_type = fixed_tags[i][2:]
                if i == 0 or (not fixed_tags[i-1].startswith('B-' + entity_type) and 
                            not fixed_tags[i-1].startswith('I-' + entity_type)):
                    fixed_tags[i] = 'B-' + entity_type
        return fixed_tags

    def smart_chunk_text(self, item: dict, max_length: int = 510) -> List[dict]:
        text = item['text']
        original_labels = item.get('label', [])
        sentences = sent_tokenize(text)
        chunks = []
        current_chunk_sentences = []
        current_token_count = 0 
        search_cursor = 0
        
        def flush_chunk(sent_buffer):
            if not sent_buffer: return
            chunk_start = sent_buffer[0][0]
            chunk_end = sent_buffer[-1][1]
            chunk_text = text[chunk_start:chunk_end]
            new_labels = [] # Logic label g·ªëc c·ªßa b·∫°n (n·∫øu c√≥)
            chunks.append({"text": chunk_text, "label": new_labels})

        for sent in sentences:
            sent_start = text.find(sent, search_cursor)
            if sent_start == -1: sent_start = search_cursor
            sent_end = sent_start + len(sent)
            sent_token_ids = self.tokenizer.encode(sent, add_special_tokens=False)
            sent_token_count = len(sent_token_ids)
            
            if current_token_count + sent_token_count <= max_length:
                current_chunk_sentences.append((sent_start, sent_end, sent))
                current_token_count += sent_token_count
            else:
                flush_chunk(current_chunk_sentences)
                current_chunk_sentences = [(sent_start, sent_end, sent)]
                current_token_count = sent_token_count
            search_cursor = sent_end

        if current_chunk_sentences:
            flush_chunk(current_chunk_sentences)
        return chunks

    def _predict_chunk(self, text: str) -> List[Dict]:
        inputs = self.tokenizer(text, padding="max_length", truncation=True, max_length=self.MAX_LENGTH, return_offsets_mapping=True, return_tensors="pt")
        offset_mapping = inputs.pop("offset_mapping")
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = outputs.logits.argmax(dim=-1)
            confidences = torch.softmax(outputs.logits, dim=-1).max(dim=-1)[0]
        
        predictions = predictions.cpu().numpy()[0]
        confidences = confidences.cpu().numpy()[0]
        offset_mapping = offset_mapping.cpu().numpy()[0]
        
        raw_labels = []
        valid_indices = []
        
        for i, (pred_id, conf, (start, end)) in enumerate(zip(predictions, confidences, offset_mapping)):
            if start == end: continue 
            label = self.model.config.id2label[int(pred_id)]
            raw_labels.append(label)
            valid_indices.append((i, conf, start, end))
        
        fixed_labels = self.fix_bio_tags(raw_labels)
        entities = []
        current_entity = None
        
        for (i, conf, start, end), label in zip(valid_indices, fixed_labels):
            if label == "O":
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
                continue
            
            entity_type = label[2:]
            if label.startswith("B-"):
                if current_entity: entities.append(current_entity)
                current_entity = {
                    "text": text[start:end], "label": entity_type,
                    "start": int(start), "end": int(end), "confidence": float(conf)
                }
            elif label.startswith("I-") and current_entity and current_entity["label"] == entity_type:
                current_entity["text"] += text[start:end]
                current_entity["end"] = int(end)
                current_entity["confidence"] = max(current_entity["confidence"], float(conf))
        
        if current_entity: entities.append(current_entity)
        return entities

    def predict_text(self, text: str) -> Dict:
        chunks = self.smart_chunk_text({'text': text}, max_length=self.MAX_LENGTH)
        all_entities = []
        current_offset = 0
        for chunk in chunks:
            chunk_text = chunk["text"]
            chunk_entities = self._predict_chunk(chunk_text)
            
            chunk_start = text.find(chunk_text, current_offset)
            if chunk_start != -1:
                for entity in chunk_entities:
                    entity["start"] += chunk_start
                    entity["end"] += chunk_start
                    all_entities.append(entity)
                current_offset = chunk_start + len(chunk_text)
        
        return {"text": text, "entities": all_entities}

# ==========================================
# PH·∫¶N 2: GRADIO VISUALIZATION HELPERS
# ==========================================

def generate_token_html(tokens, labels, confidences):
    """
    H√†m helper t·∫°o HTML (ƒê√£ th√™m m√†u cho ADDR)
    """
    html_parts = []
    
    # === C·∫¨P NH·∫¨T B·∫¢NG M√ÄU T·∫†I ƒê√ÇY ===
    colors = {
        "PER": "#ffdad9",   # ƒê·ªè nh·∫°t
        "ORG": "#d7e3ff",   # Xanh d∆∞∆°ng nh·∫°t
        "ADDR": "#e9dff7",  # T√≠m nh·∫°t (M·ªöI TH√äM)
        "O": "#f5f5f5"      # X√°m (Kh√¥ng ph·∫£i entity)
    }
    
    # M√†u vi·ªÅn t∆∞∆°ng ·ª©ng (ƒë·∫≠m h∆°n n·ªÅn ch√∫t)
    border_colors = {
        "PER": "#ffb4ab",
        "ORG": "#abc7ff",
        "ADDR": "#d0bcff",  # Vi·ªÅn T√≠m (M·ªöI TH√äM)
        "O": "#cccccc"
    }

    for token, label, conf in zip(tokens, labels, confidences):
        if token in ["<s>", "</s>", "<pad>"]:
            continue
            
        # T√°ch l·∫•y ph·∫ßn type (v√≠ d·ª• B-ADDR -> ADDR)
        entity_type = label.split("-")[-1] if "-" in label else label
        
        # L·∫•y m√†u, n·∫øu kh√¥ng c√≥ trong list th√¨ l·∫•y m√†u c·ªßa "O"
        bg_color = colors.get(entity_type, colors["O"])
        bd_color = border_colors.get(entity_type, border_colors["O"])
        
        tag_style = "font-size: 0.65em; opacity: 0.7; font-weight: bold; display: block; margin-top: 2px;"
        token_style = "font-family: 'Consolas', 'Monaco', monospace; font-size: 1.1em; font-weight: 600;"
        
        # Box style (V·∫´n gi·ªØ color: #333333 ƒë·ªÉ ch·ªëng m√π m√†u tr√™n Darkmode)
        box_style = (
            f"display: inline-block; text-align: center; margin: 3px; padding: 4px 8px; "
            f"background-color: {bg_color}; border: 1px solid {bd_color}; border-radius: 6px; "
            f"line-height: 1.2; vertical-align: top; position: relative; "
            f"min-width: 20px; color: #333333;" 
        )
        
        tooltip_attr = f'title="Tag: {label} | Conf: {conf:.4f}"'

        html_parts.append(
            f'<div style="{box_style}" {tooltip_attr}>'
            f'<span style="{token_style}">{token}</span>'
            f'<span style="{tag_style}">{label}</span>'
            f'</div>'
        )
        
    return "".join(html_parts)

def visualize_token_map(text):
    """
    Hi·ªÉn th·ªã Full Text d∆∞·ªõi d·∫°ng Token Blocks gi·ªëng Tokenizer Playground
    nh∆∞ng c√≥ th√™m m√†u Entity v√† Tag d·ª± ƒëo√°n.
    """
    if not text.strip():
        return ""

    # 1. Chunking text ƒë·ªÉ x·ª≠ l√Ω vƒÉn b·∫£n d√†i
    chunks = predictor.smart_chunk_text({'text': text}, max_length=predictor.MAX_LENGTH)
    
    full_html = ['<div style="font-family: sans-serif; padding: 10px; line-height: 2.5;">']
    
    for chunk in chunks:
        chunk_text = chunk["text"]
        
        # 2. Inference t·ª´ng chunk
        inputs = predictor.tokenizer(chunk_text, truncation=True, max_length=512, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
        with torch.no_grad():
            outputs = predictor.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()
            confidences = torch.softmax(outputs.logits, dim=2).max(dim=2)[0][0].cpu().numpy()
            
        # 3. L·∫•y tokens v√† mapping nh√£n
        tokens = predictor.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        id2label = predictor.model.config.id2label
        
        labels = []
        for p in predictions:
            l = str(id2label[p]) if id2label else str(p)
            labels.append("O" if l == "0" or l == "O" else l)
            
        # 4. Generate HTML cho chunk n√†y
        full_html.append(generate_token_html(tokens, labels, confidences))
        
        # Th√™m d·∫•u ng·∫Øt d√≤ng visual gi·ªØa c√°c chunk (n·∫øu mu·ªën)
        # full_html.append('<div style="width: 100%; height: 10px;"></div>')
        
    full_html.append('</div>')
    return "".join(full_html)

# Kh·ªüi t·∫°o predictor to√†n c·ª•c
# L∆ØU √ù: ƒê·ªïi ƒë∆∞·ªùng d·∫´n model ·ªü ƒë√¢y n·∫øu ch·∫°y local
MODEL_NAME = "visobert_model/final" 
# N·∫øu b·∫°n ch∆∞a train NER, model n√†y s·∫Ω load weight random cho head classification
# N·∫øu b·∫°n c√≥ folder model ƒë√£ train, thay path v√†o ƒë√¢y: e.g., "visobert_model/final"
try:
    predictor = VietnameseNERPredictor(MODEL_NAME)
except Exception as e:
    print(f"Error loading model: {e}")
    print("Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n model.")

def inspect_tokens_detailed(text):
    """
    H√†m debug Tokenizer c√≥ h·ªó tr·ª£ Chunking.
    N√≥ s·∫Ω ch·∫°y y h·ªát logic c·∫Øt ƒëo·∫°n c·ªßa model ch√≠nh ƒë·ªÉ b·∫°n ki·ªÉm tra xem
    c√¢u b·ªã c·∫Øt ·ªü ƒë√¢u, c√≥ b·ªã m·∫•t ng·ªØ nghƒ©a kh√¥ng.
    """
    # 1. S·ª≠ d·ª•ng ch√≠nh logic chunking c·ªßa class predictor
    chunks = predictor.smart_chunk_text({'text': text}, max_length=predictor.MAX_LENGTH)
    
    all_rows = []
    
    # 2. L·∫∑p qua t·ª´ng chunk ƒë·ªÉ inspect
    for chunk_idx, chunk in enumerate(chunks):
        chunk_text = chunk["text"]
        
        # Tokenize l·∫°i t·ª´ng chunk (ƒë·ªÉ l·∫•y input_ids v√† attention_mask cho model)
        inputs = predictor.tokenizer(chunk_text, truncation=True, max_length=512, return_tensors="pt")
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
        # Predict t·ª´ng chunk
        with torch.no_grad():
            outputs = predictor.model(**inputs)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=2)[0].cpu().numpy()
            confidences = torch.softmax(logits, dim=2).max(dim=2)[0][0].cpu().numpy()

        # Convert IDs to Tokens
        tokens = predictor.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        id2label = predictor.model.config.id2label

        # 3. L∆∞u th√¥ng tin t·ª´ng token v√†o list
        for i, (token, pred_id, conf) in enumerate(zip(tokens, predictions, confidences)):
            # Fix l·ªói label 0 vs O
            raw_label = str(id2label[pred_id]) if id2label else str(pred_id)
            label = "O" if raw_label == "0" or raw_label == "O" else raw_label
            
            # Highlight logic
            is_entity = label != "O" and token not in ["<s>", "</s>", "<pad>"]
            
            all_rows.append({
                "Chunk #": chunk_idx + 1,      # ƒê·ªÉ bi·∫øt token thu·ªôc ƒëo·∫°n c·∫Øt th·ª© m·∫•y
                "Index": i,                    # V·ªã tr√≠ trong chunk
                "Token": token,
                "Token ID": inputs["input_ids"][0][i].item(),
                "Predicted Tag": label,
                "Confidence": round(float(conf), 4),
                "Is Entity": "‚úÖ" if is_entity else ""
            })
            
    # 4. Tr·∫£ v·ªÅ DataFrame g·ªôp
    return pd.DataFrame(all_rows)

def visualize_ner(text):
    """
    Phi√™n b·∫£n Fix l·ªói hi·ªÉn th·ªã Gradio:
    1. √âp ki·ªÉu m·∫°nh m·∫Ω (str, int) ƒë·ªÉ tr√°nh numpy types.
    2. In ra format cu·ªëi c√πng ƒë·ªÉ debug.
    """
    if not text.strip():
        return []

    # 1. L·∫•y k·∫øt qu·∫£ t·ª´ model
    result = predictor.predict_text(text)
    entities = result['entities']
    
    # Sort entity theo v·ªã tr√≠
    entities.sort(key=lambda x: x['start'])

    formatted_output = []
    cursor = 0
    
    # 2. Loop ƒë·ªÉ c·∫Øt gh√©p chu·ªói
    for ent in entities:
        # √âp ki·ªÉu int tuy·ªát ƒë·ªëi cho start/end
        start = int(ent['start'])
        end = int(ent['end'])
        
        # √âp ki·ªÉu string tuy·ªát ƒë·ªëi cho label
        label = str(ent['label'])
        
        # C·∫Øt text (ƒë·∫£m b·∫£o index n·∫±m trong gi·ªõi h·∫°n)
        start = max(0, start)
        end = min(len(text), end)
        
        # Ph·∫ßn Text th∆∞·ªùng (Label l√† None)
        if start > cursor:
            sub_text = str(text[cursor:start]) # √âp ki·ªÉu str
            if sub_text:
                formatted_output.append((sub_text, None))
        
        # Ph·∫ßn Entity (Label c√≥ gi√° tr·ªã)
        ent_text = str(text[start:end]) # √âp ki·ªÉu str
        if ent_text:
            formatted_output.append((ent_text, label))
            
        cursor = end
        
    # Ph·∫ßn Text c√≤n l·∫°i sau entity cu·ªëi
    if cursor < len(text):
        remainder = str(text[cursor:])
        formatted_output.append((remainder, None))

    
    return formatted_output
# ==========================================
# PH·∫¶N 3: GRADIO UI
# ==========================================

with gr.Blocks(title="Vietnamese NER Inspector") as demo:
    gr.Markdown("# üïµÔ∏è Vietnamese NER Inspector (ViSoBERT)")
    gr.Markdown("Tool n√†y gi√∫p visualize k·∫øt qu·∫£ NER v√† inspect chi ti·∫øt c√°ch model tokenize d·ªØ li·ªáu.")
    
    with gr.Row():
        input_text = gr.Textbox(
            label="Input Text", 
            lines=5, 
            placeholder="Nh·∫≠p vƒÉn b·∫£n ti·∫øng Vi·ªát v√†o ƒë√¢y...",
            value="B√°c sƒ© Nguy·ªÖn VƒÉn A l√†m vi·ªác t·∫°i B·ªánh vi·ªán Ch·ª£ R·∫´y TP.HCM."
        )
    
    with gr.Row():
        btn_submit = gr.Button("üöÄ Analyze", variant="primary")

    with gr.Tabs():
        with gr.TabItem("üé® Visual Entities"):
            gr.Markdown("*K·∫øt qu·∫£ sau khi ƒë√£ gh√©p c√°c tokens (Logic c·ªßa `predict_text`)*")
            output_highlight = gr.HighlightedText(
                label="Named Entities",
                combine_adjacent=True,
                show_legend=True,
            )
            
        with gr.TabItem("üî¨ Token Inspector"):
            gr.Markdown("""
            *B·∫£ng n√†y hi·ªÉn th·ªã c√°ch **Tokenizer** c·∫Øt t·ª´ v√† **Raw Tag** m√† model d·ª± ƒëo√°n cho t·ª´ng token.*
            - Quan s√°t c·ªôt `Token` ƒë·ªÉ xem sub-word (v√≠ d·ª• `_H·ªì`, `_Ch√≠`).
            - Quan s√°t c·ªôt `Predicted Tag` ƒë·ªÉ xem B-TAG, I-TAG.
            """)
            output_df = gr.Dataframe(
                # Th√™m c·ªôt "Chunk #" v√†o ƒë·∫ßu danh s√°ch headers
                headers=["Chunk #", "Index", "Token", "Token ID", "Predicted Tag", "Confidence", "Is Entity"],
                interactive=False
            )

        with gr.TabItem("üß© Token Map & NER"):
            gr.Markdown("""
            *Giao di·ªán n√†y hi·ªÉn th·ªã t·ª´ng token trong m·ªôt h·ªôp ri√™ng bi·ªát.*
            - **M√†u n·ªÅn**: Lo·∫°i th·ª±c th·ªÉ (ƒê·ªè=PER, Xanh=ORG...).
            - **D√≤ng tr√™n**: Token (c√≥ d·∫•u `_` n·∫øu l√† ƒë·∫ßu t·ª´).
            - **D√≤ng d∆∞·ªõi**: Tag d·ª± ƒëo√°n (B-..., I-...).
            - *R√™ chu·ªôt v√†o h·ªôp ƒë·ªÉ xem ƒë·ªô tin c·∫≠y (Confidence score).*
            """)
            # D√πng gr.HTML ƒë·ªÉ render
            output_token_map = gr.HTML(label="Token Map Visualization")


    # S·ª± ki·ªán click
    btn_submit.click(
        fn=lambda x: (visualize_ner(x), inspect_tokens_detailed(x), visualize_token_map(x)),
        inputs=[input_text],
        outputs=[output_highlight, output_df, output_token_map]
    )

if __name__ == "__main__":
    demo.launch()