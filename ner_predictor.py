#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple Vietnamese NER Prediction
Chá»‰ cÃ³ function cÆ¡ báº£n Ä‘á»ƒ dá»± Ä‘oÃ¡n NER tá»« model Ä‘Ã£ train
"""

import torch
import os
import re
from transformers import AutoTokenizer, AutoModelForTokenClassification
from typing import List, Dict
from underthesea import sent_tokenize as vn_sent_tokenize


class VietnameseNERPredictor:
    """Simple Vietnamese NER Predictor"""
    
    def __init__(self, model_path: str = "mdeberta_ner_model/final"):
        """Initialize predictor with model"""
        self.model_path = os.path.join(os.path.dirname(__file__), model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)
        self.model.eval()

        self._TOKENIZER = self.tokenizer # alias
        self.MAX_LENGTH = 512
        
        # Move to GPU if available
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print("Model loaded on GPU")
        else:
            print("Model loaded on CPU")

    def _vn_sentences(self, s: str) -> List[str]:
        raise NotImplementedError
        return [t.strip() for t in vn_sent_tokenize(s) if t.strip()]
    
    def remove_hrules(self, text: str) -> str:
        """XÃ³a chuá»—i káº» ngang (----, â€”â€”â€”â€”, ____) â‰¥ 4 kÃ½ tá»±, giá»¯ nguyÃªn cÃ¡c kÃ½ tá»± khÃ¡c."""
        return re.sub(r"[-â€“â€”_]{4,}", "", text)
    
    def smart_chunk_text(self, text: str, max_length: int = 512) -> List[str]:
        """
        Chunking vÄƒn báº£n tá»‘i Æ°u Ä‘á»ƒ Ä‘áº¡t gáº§n max_length (tÃ­nh theo TOKEN)
        
        Quy táº¯c 1: CÃ¡c má»¥c trong danh sÃ¡ch cÃ³ thá»ƒ ghÃ©p láº¡i vá»›i nhau
        Quy táº¯c 2: CÃ¡c khá»‘i liÃªn há»‡ vÃ  siÃªu dá»¯ liá»‡u lÃ  khÃ´ng thá»ƒ chia cáº¯t  
        Quy táº¯c 3: Xá»­ lÃ½ Hashtag
        """
        # Helper: Ä‘áº¿m token (ká»ƒ cáº£ special tokens Ä‘á»ƒ sÃ¡t giá»›i háº¡n mÃ´ hÃ¬nh)
        def _tok_len(s: str) -> int:
            # LÆ°u Ã½: encode cáº£ special tokens Ä‘á»ƒ má»—i chunk khi Ä‘Æ°a vÃ o model khÃ´ng vÆ°á»£t quÃ¡ max_length
            return len(self._TOKENIZER.encode(s, add_special_tokens=True))

        # Validate input
        if not text or not text.strip():
            return []
        
        chunks: List[str] = []
        
        try:
            # Quy táº¯c 3: Xá»­ lÃ½ hashtag - tÃ¡ch ra vÃ  lÆ°u trá»¯ riÃªng
            hashtag_pattern = r'#\w+'
            hashtags = re.findall(hashtag_pattern, text)
            text_without_hashtags = re.sub(hashtag_pattern, '', text).strip()
            
            # XoÃ¡ cÃ¡c chuá»—i káº» ngang trÆ°á»›c khi chunk
            text_without_hashtags = self.remove_hrules(text_without_hashtags)
            
            # TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c segments theo loáº¡i
            segments = []
            lines = text_without_hashtags.split('\n')
            
            # CÃ¡c tá»« khÃ³a Ä‘á»ƒ nháº­n diá»‡n khá»‘i liÃªn há»‡
            contact_keywords = ['hotline', 'Ä‘á»‹a chá»‰', 'website', 'email', 'zalo', 'facebook', 'tel', 'fax', 'tá»•ng Ä‘Ã i']
            
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                if not line:
                    i += 1
                    continue
                    
                # Quy táº¯c 2: PhÃ¡t hiá»‡n khá»‘i liÃªn há»‡/siÃªu dá»¯ liá»‡u
                line_lower = line.lower()
                is_contact_line = any(keyword in line_lower for keyword in contact_keywords)
                has_address = bool(re.search(r'(sá»‘\s+\d+|Ä‘Æ°á»ng|phÆ°á»ng|quáº­n|tá»‰nh|thÃ nh phá»‘)', line_lower))
                has_org_pattern = bool(re.search(r'<ORG>.*?</ORG>', line))
                has_addr_pattern = bool(re.search(r'<ADDR>.*?</ADDR>', line))
                has_phone_pattern = bool(re.search(r'<PER>.*?</PER>', line))
                
                if is_contact_line or has_address or has_org_pattern or has_addr_pattern or has_phone_pattern:
                    # Gom khá»‘i liÃªn há»‡ liá»n nhau (khÃ´ng cáº¯t nhá»)
                    contact_block = line
                    j = i + 1
                    while j < len(lines):
                        next_line = lines[j].strip()
                        if not next_line:
                            j += 1
                            continue
                            
                        next_line_lower = next_line.lower()
                        is_next_contact = any(keyword in next_line_lower for keyword in contact_keywords)
                        has_next_address = bool(re.search(r'(sá»‘\s+\d+|Ä‘Æ°á»ng|phÆ°á»ng|quáº­n|tá»‰nh|thÃ nh phá»‘)', next_line_lower))
                        has_next_org = bool(re.search(r'<ORG>.*?</ORG>', next_line))
                        has_next_addr = bool(re.search(r'<ADDR>.*?</ADDR>', next_line))
                        has_next_phone = bool(re.search(r'<PER>.*?</PER>', next_line))
                        
                        if (is_next_contact or has_next_address or has_next_org or 
                            has_next_addr or has_next_phone or
                            re.match(r'^\d+', next_line) or  # cÃ³ thá»ƒ lÃ  sá»‘ Ä‘iá»‡n thoáº¡i
                            'www.' in next_line_lower or '.com' in next_line_lower):  # website
                            contact_block += " " + next_line
                            j += 1
                        else:
                            break
                    
                    segments.append({"type": "contact", "content": contact_block})
                    i = j - 1
                    
                # Quy táº¯c 1: Xá»­ lÃ½ má»¥c danh sÃ¡ch
                elif re.match(r'^[\-\*]\s+', line) or re.match(r'^\d+[\.\)]\s+', line):
                    list_item = line
                    j = i + 1
                    while j < len(lines) and lines[j].strip():
                        next_line = lines[j].strip()
                        if not (re.match(r'^[\-\*]\s+', next_line) or re.match(r'^\d+[\.\)]\s+', next_line)):
                            next_line_lower = next_line.lower()
                            is_next_contact = any(keyword in next_line_lower for keyword in contact_keywords)
                            if not is_next_contact:
                                list_item += " " + next_line
                                j += 1
                            else:
                                break
                        else:
                            break
                    segments.append({"type": "list_item", "content": list_item})
                    i = j - 1

                else:
                    segments.append({"type": "normal", "content": line})
                i += 1
            
            # GhÃ©p segments thÃ nh chunks theo token
            current_chunk = ""
            segment_buffer = []
            
            def _join_contents(items) -> str:
                return " ".join([s["content"] for s in items]).strip()

            def try_add_segment(segment) -> bool:
                # Thá»­ thÃªm vÃ o buffer + current_chunk, kiá»ƒm tra theo token
                test_content = _join_contents(segment_buffer + [segment])
                test_chunk = (current_chunk + " " + test_content).strip() if current_chunk else test_content
                return _tok_len(test_chunk) <= max_length
            
            def flush_buffer():
                nonlocal current_chunk, segment_buffer
                if not segment_buffer:
                    return
                buffer_content = _join_contents(segment_buffer)
                if current_chunk:
                    combined = (current_chunk + " " + buffer_content).strip()
                    if _tok_len(combined) <= max_length:
                        current_chunk = combined
                    else:
                        chunks.append(current_chunk.strip())
                        current_chunk = buffer_content
                else:
                    current_chunk = buffer_content
                segment_buffer = []
            
            def flush_current():
                nonlocal current_chunk
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
            
            for segment in segments:
                if segment["type"] == "contact":
                    flush_buffer()
                    flush_current()
                    if _tok_len(segment["content"]) <= max_length:
                        chunks.append(segment["content"].strip())
                    else:
                        # cáº¯t contact â€œan toÃ nâ€ theo cÃ¢u/cá»¥m (giá»¯ cÃ¡c pháº§n liá»n ká»)
                        def _split_contact(s: str) -> List[str]:
                            out, cur = [], ""
                            sents = self._vn_sentences(s) or [s]
                            for sent in sents:
                                cand = (cur + (" " if cur else "") + sent).strip()
                                if _tok_len(cand) <= max_length:
                                    cur = cand
                                else:
                                    if cur:
                                        out.append(cur.strip())
                                    # tÃ¡ch má»‡nh Ä‘á», rá»“i fallback theo tá»« náº¿u cáº§n
                                    clauses = re.split(r',\s+|;\s+|\s+\-\s+|\s+\|\s+', sent)
                                    buf = ""
                                    for c in clauses:
                                        c = c.strip()
                                        if not c:
                                            continue
                                        cand2 = (buf + (", " if buf else "") + c).strip()
                                        if _tok_len(cand2) <= max_length:
                                            buf = cand2
                                        else:
                                            if buf:
                                                out.append(buf.strip())
                                            wbuf = ""
                                            for w in c.split():
                                                cand3 = (wbuf + (" " if wbuf else "") + w).strip()
                                                if _tok_len(cand3) <= max_length:
                                                    wbuf = cand3
                                                else:
                                                    if wbuf:
                                                        out.append(wbuf.strip())
                                                    wbuf = w
                                            if wbuf:
                                                out.append(wbuf.strip())
                                            buf = ""
                                    if buf:
                                        out.append(buf.strip())
                                    cur = ""
                            if cur:
                                out.append(cur.strip())
                            return out

                        chunks.extend(_split_contact(segment["content"]))
            
            # Flush pháº§n cÃ²n láº¡i
            flush_buffer()
            flush_current()
            
            # ThÃªm hashtag (náº¿u cÃ³) vÃ o cÃ¡c chunk mÃ  váº«n khÃ´ng vÆ°á»£t token
            if hashtags and chunks:
                hashtag_text = " " + " ".join(hashtags)
                # thá»­ vÃ o chunk cuá»‘i trÆ°á»›c
                candidate = (chunks[-1] + hashtag_text).strip()
                if _tok_len(candidate) <= max_length:
                    chunks[-1] = candidate
                else:
                    # thá»­ lÃ¹i dáº§n cÃ¡c chunk trÆ°á»›c Ä‘Ã³
                    placed = False
                    for k in range(len(chunks) - 2, -1, -1):
                        test = (chunks[k] + hashtag_text).strip()
                        if _tok_len(test) <= max_length:
                            chunks[k] = test
                            placed = True
                            break
                    if not placed:
                        chunks.append(" ".join(hashtags))
            elif hashtags:
                chunks.append(" ".join(hashtags))
            
            # Merge cÃ¡c chunk cÃ²n nhá» náº¿u gá»™p láº¡i khÃ´ng vÆ°á»£t token
            optimized_chunks: List[str] = []
            for ch in chunks:
                if optimized_chunks:
                    merged = (optimized_chunks[-1] + " " + ch).strip()
                    if _tok_len(merged) <= max_length:
                        optimized_chunks[-1] = merged
                    else:
                        optimized_chunks.append(ch)
                else:
                    optimized_chunks.append(ch)
            
            # Xá»­ lÃ½ cÃ¡c chunk váº«n quÃ¡ dÃ i (token) báº±ng cÃ¡ch tÃ¡ch cÃ¢u/cá»¥m (regex)
            final_chunks: List[str] = []
            for ch in optimized_chunks:
                if _tok_len(ch) <= max_length:
                    final_chunks.append(ch)
                    continue
                
                # TÃ¡ch cÃ¢u, sau Ä‘Ã³ gá»™p láº¡i theo token
                sentences = self._vn_sentences(ch)
                if not sentences:
                    sentences = [ch]
                
                current_subchunk = ""
                for sent in sentences:
                    sent = sent.strip()
                    if not sent:
                        continue
                    # thá»­ thÃªm cÃ¢u vÃ o subchunk (thÃªm ". " Ä‘á»ƒ giá»¯ dáº¥u káº¿t thÃºc)
                    test_sub = (current_subchunk + (" " if current_subchunk else "") + sent).strip()
                    if _tok_len(test_sub) <= max_length:
                        current_subchunk = test_sub
                    else:
                        if current_subchunk:
                            final_chunks.append(current_subchunk.strip())
                        # náº¿u cÃ¢u quÃ¡ dÃ i so vá»›i max_length, tÃ¡ch tiáº¿p theo clause
                        if _tok_len(sent) > max_length:
                            def find_best_split_point(text, max_tokens):
                                """TÃ¬m Ä‘iá»ƒm cáº¯t tá»‘t nháº¥t Ä‘á»ƒ giá»¯ nguyÃªn Ã½ nghÄ©a"""
                                # Æ¯u tiÃªn cáº¯t á»Ÿ cuá»‘i cÃ¢u hoÃ n chá»‰nh
                                sentence_ends = [m.end() for m in re.finditer(r'[.!?]\s+', text)]
                                for pos in reversed(sentence_ends):
                                    if _tok_len(text[:pos]) <= max_tokens:
                                        return pos
                                
                                # Náº¿u khÃ´ng cÃ³, cáº¯t á»Ÿ dáº¥u pháº©y + tá»« ná»‘i
                                clause_ends = [m.end() for m in re.finditer(r'[,;]\s+(?:vÃ |hoáº·c|nhÆ°ng|mÃ |nÃªn|náº¿u|khi|Ä‘á»ƒ)\s+', text)]
                                for pos in reversed(clause_ends):
                                    if _tok_len(text[:pos]) <= max_tokens:
                                        return pos
                                
                                # Cuá»‘i cÃ¹ng má»›i cáº¯t á»Ÿ dáº¥u pháº©y thÆ°á»ng
                                comma_ends = [m.end() for m in re.finditer(r',\s+', text)]
                                for pos in reversed(comma_ends):
                                    if _tok_len(text[:pos]) <= max_tokens:
                                        return pos
                                
                                return None

                            # Sá»­ dá»¥ng hÃ m nÃ y thay vÃ¬ split Ä‘Æ¡n giáº£n
                            clauses = []
                            remaining = sent
                            while remaining and _tok_len(remaining) > max_length:
                                split_pos = find_best_split_point(remaining, max_length)
                                if split_pos:
                                    clauses.append(remaining[:split_pos].strip())
                                    remaining = remaining[split_pos:].strip()
                                else:
                                    # Fallback: cáº¯t theo tá»«
                                    words = remaining.split()
                                    current = ""
                                    for word in words:
                                        test = (current + " " + word).strip() if current else word
                                        if _tok_len(test) <= max_length:
                                            current = test
                                        else:
                                            if current:
                                                clauses.append(current)
                                            current = word
                                            break
                                    remaining = " ".join(words[len(current.split()):])
                            if remaining:
                                clauses.append(remaining)
                            current_subchunk = ""
                            for clause in clauses:
                                clause = clause.strip()
                                if not clause:
                                    continue
                                test_clause = (current_subchunk + (", " if current_subchunk else "") + clause).strip()
                                if _tok_len(test_clause) <= max_length:
                                    current_subchunk = test_clause
                                else:
                                    if current_subchunk:
                                        final_chunks.append(current_subchunk.strip())
                                    current_subchunk = clause
                        else:
                            current_subchunk = sent
                
                if current_subchunk:
                    final_chunks.append(current_subchunk.strip())
            
            # Loáº¡i bá» chunk rá»—ng
            return [c for c in final_chunks if c.strip()]
    
        except Exception:
            # Fallback: tÃ¡ch theo cÃ¢u trÆ°á»›c, sau Ä‘Ã³ má»›i theo tá»«
            try:
                sentences = self._vn_sentences(text)
                chunks = []
                current_chunk = ""
                
                for sent in sentences:
                    sent = sent.strip()
                    if not sent:
                        continue
                        
                    test_chunk = (current_chunk + " " + sent).strip() if current_chunk else sent
                    if _tok_len(test_chunk) <= max_length:
                        current_chunk = test_chunk
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        
                        # Náº¿u cÃ¢u Ä‘Æ¡n láº» váº«n quÃ¡ dÃ i, tÃ¡ch theo tá»« cáº©n tháº­n
                        if _tok_len(sent) > max_length:
                            words = sent.split()
                            word_chunk = ""
                            for word in words:
                                test_word = (word_chunk + " " + word).strip() if word_chunk else word
                                if _tok_len(test_word) <= max_length:
                                    word_chunk = test_word
                                else:
                                    if word_chunk:
                                        chunks.append(word_chunk.strip())
                                    word_chunk = word
                            if word_chunk:
                                current_chunk = word_chunk.strip()
                            else:
                                current_chunk = ""
                        else:
                            current_chunk = sent
                
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                return [c for c in chunks if c.strip()]
            
            except:
                # Ultimate fallback: tráº£ vá» text gá»‘c náº¿u khÃ´ng thá»ƒ xá»­ lÃ½
                if _tok_len(text) <= max_length:
                    return [text.strip()]
                else:
                    # Cáº¯t cá»©ng theo tá»« nhÆ°ng cáº©n tháº­n hÆ¡n
                    words = text.split()
                    chunks = []
                    current = ""
                    
                    for word in words:
                        test = (current + " " + word).strip() if current else word
                        if _tok_len(test) <= max_length:
                            current = test
                        else:
                            if current:
                                chunks.append(current.strip())
                                current = word
                            else:
                                # Tá»« Ä‘Æ¡n quÃ¡ dÃ i, buá»™c pháº£i cáº¯t
                                chunks.append(word)
                                current = ""
                    
                    if current:
                        chunks.append(current.strip())
                    
                    return [c for c in chunks if c.strip()]

    def fix_bio_tags(self, tags):
        """
        Fix invalid BIO tag sequences by converting first I- tags without preceding B- to B- tags
        """
        fixed_tags = list(tags)
        
        for i in range(len(fixed_tags)):
            if fixed_tags[i].startswith('I-'):
                entity_type = fixed_tags[i][2:]
                
                if i == 0 or (not fixed_tags[i-1].startswith('B-' + entity_type) and 
                            not fixed_tags[i-1].startswith('I-' + entity_type)):
                    fixed_tags[i] = 'B-' + entity_type
        
        return fixed_tags

    def predict_text(self, text: str) -> Dict:
        """
        Dá»± Ä‘oÃ¡n entities trong vÄƒn báº£n
        
        Args:
            text: VÄƒn báº£n cáº§n dá»± Ä‘oÃ¡n
            
        Returns:
            Dictionary chá»©a káº¿t quáº£ dá»± Ä‘oÃ¡n
        """
        chunks = self.smart_chunk_text(text)
        for idx, chunk in enumerate(chunks):
            print(f"chunk {idx}: {chunk}")
        all_entities = []
        current_offset = 0
        for chunk in chunks:
            chunk_entities = self._predict_chunk(chunk)
            
            chunk_start = text.find(chunk, current_offset)
            if chunk_start != -1:
                for entity in chunk_entities:
                    entity["start"] += chunk_start
                    entity["end"] += chunk_start
                    all_entities.append(entity)
            
            current_offset = text.find(chunk, current_offset) + len(chunk)
        
        entities_by_type = {}
        for entity in all_entities:
            label = entity["label"]
            if label not in entities_by_type:
                entities_by_type[label] = []
            entities_by_type[label].append(entity)
        
        return {
            "text": text,
            "entities": all_entities,
            "entities_by_type": entities_by_type,
            "total_entities": len(all_entities),
            "entity_types": list(entities_by_type.keys())
        }

    def _predict_chunk(self, text: str) -> List[Dict]:
        # Tokenize
        inputs = self.tokenizer(
            text, 
            padding="max_length",
            truncation=True,
            max_length=self.MAX_LENGTH,
            return_offsets_mapping=True,
            return_tensors="pt",
        )
        offset_mapping = inputs.pop("offset_mapping")
        
        # Move to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = outputs.logits.argmax(dim=-1)
            confidences = torch.softmax(outputs.logits, dim=-1).max(dim=-1)[0]
        
        # Convert to CPU
        predictions = predictions.cpu().numpy()[0]
        confidences = confidences.cpu().numpy()[0]
        offset_mapping = offset_mapping.cpu().numpy()[0]
        
        # Convert predictions to labels
        raw_labels = []
        valid_indices = []
        
        for i, (pred_id, conf, (start, end)) in enumerate(zip(predictions, confidences, offset_mapping)):
            if start == end:  # Skip special tokens
                continue
            
            label = self.model.config.id2label[int(pred_id)]
            raw_labels.append(label)
            valid_indices.append((i, conf, start, end))
        
        # Fix BIO tag sequences
        fixed_labels = self.fix_bio_tags(raw_labels)
        
        # Extract entities from fixed labels
        entities = []
        current_entity = None
        
        for (i, conf, start, end), label in zip(valid_indices, fixed_labels):
            if label == "O":
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
                continue
            
            entity_type = label[2:]  # Remove B- or I-
            
            if label.startswith("B-"):
                if current_entity:
                    entities.append(current_entity)
                
                current_entity = {
                    "text": text[start:end],
                    "label": entity_type,
                    "start": int(start),
                    "end": int(end),
                    "confidence": float(conf)
                }
            elif label.startswith("I-") and current_entity and current_entity["label"] == entity_type:
                current_entity["text"] += text[start:end]
                current_entity["end"] = int(end)
                current_entity["confidence"] = max(current_entity["confidence"], float(conf))
        
        if current_entity:
            entities.append(current_entity)
        
        return entities

    def format_results(self, results: Dict, show_confidence: bool = True) -> str:
        """Format káº¿t quáº£ Ä‘á»ƒ hiá»ƒn thá»‹"""
        output = []
        output.append(f"ğŸ“ Text: {results['text'][:100]}{'...' if len(results['text']) > 100 else ''}")
        output.append(f"ğŸ“Š Found {results['total_entities']} entities")
        output.append("")
        
        if results['entities']:
            output.append("ğŸ·ï¸ Detected Entities:")
            for i, entity in enumerate(results['entities'], 1):
                confidence_str = f" (confidence: {entity['confidence']:.3f})" if show_confidence else ""
                output.append(f"   {i}. '{entity['text']}' â†’ {entity['label']}{confidence_str}")
            
            output.append("")
            output.append("ğŸ“‹ By Category:")
            for entity_type in sorted(results['entities_by_type'].keys()):
                entities = results['entities_by_type'][entity_type]
                output.append(f"   {entity_type}: {len(entities)} entities")
                for entity in entities:
                    output.append(f"      â€¢ {entity['text']}")
        else:
            output.append("âŒ No entities detected")
        
        return "\n".join(output)

def main():
    """Test function Ä‘Æ¡n giáº£n"""
    # Simple test
    predictor = VietnameseNERPredictor()
    
    test_text ="""[MEGA LIVE 14.11] DEAL Äá»ˆNH THÃŒNH LÃŒNH - LÃ€M Äáº¸P SIÃŠU DÃNH CÃ¹ng Ca sÄ© THU THá»¦Y SÄƒn deal Äá»’NG GIÃ tá»« 99K - QuÃ  táº·ng LÃ m Ä‘áº¹p siÃªu khá»§ng----------Danh sÃ¡ch cÃ¡c dá»‹ch vá»¥ siÃªu Hot sáº½ xuáº¥t hiá»‡n trong phiÃªn live láº§n nÃ y, vá»›i má»©c giÃ¡ Äá»˜C QUYá»€N siÃªu giáº£m:- ChÄƒm SÃ³c Da Cao Cáº¥p 3in1- Dr.Vip ChÄƒm SÃ³c Da LÃ£o HoÃ¡ ECM- Dr.Vip á»¦ Tráº¯ng Face Collagen- Dr.Vip ChÄƒm SÃ³c VÃ¹ng Máº¯t ECM - XoÃ¡ nhÄƒn váº¿t chÃ¢n chim- Dr.Vip Collagen Thuá»· PhÃ¢n - á»¨c Cháº¿ Äá»‘m NÃ¢u- Dr. Acne Trá»‹ Má»¥n Chuáº©n Y Khoa- Dr.Seoul Laser Pico 5.0- Dr.Slim Giáº£m Má»¡ Exilis Detox- Dr. White Táº¯m Tráº¯ng HoÃ ng Gia- Phun mÃ y- Phun mÃ­- Phun mÃ´iNgoÃ i ra, cÃ¡c hoáº¡t Ä‘á»™ng cá»™ng hÆ°á»Ÿng táº¡i phiÃªn live: Giao lÆ°u, trÃ² chuyá»‡n, chia sáº» kiáº¿n thá»©c lÃ m Ä‘áº¹p cÃ¹ng ca sÄ© Thu Thá»§y TÆ° váº¥n & giáº£i Ä‘Ã¡p vá» dá»‹ch vá»¥ cÃ¹ng Seoul Center Tham gia minigame - Nháº­n quÃ  Ä‘á»™c quyá»n thÆ°Æ¡ng hiá»‡uTáº¥t cáº£ DEAL há»i Ä‘Ã£ sáºµn sÃ ng "lÃªn ká»‡" vÃ o lÃºc 19h00 | 14.11.2024 táº¡i FB/ Tiktok Seoul Center vÃ  Fb/tiktok ca sÄ© Thu Thá»§y Giáº£m giÃ¡ ká»‹ch sÃ n, chá»‰ cÃ³ trÃªn live Äáº·t lá»‹ch sÄƒn ngay lÃ m Ä‘áº¹p Ä‘Ã³n táº¿t cÃ¹ng Thu Thá»§y nhÃ©!-------------Há»‡ Thá»‘ng Tháº©m Má»¹ Quá»‘c Táº¿ Seoul CenterSáºµn sÃ ng láº¯ng nghe má»i Ã½ kiáº¿n cá»§a khÃ¡ch hÃ ng: 1800 3333Äáº·t lá»‹ch ngay vá»›i Top dá»‹ch vá»¥ Ä‘áº·c quyá»n: Website: Zalo: Tiktok: Youtube: Top 10 ThÆ°Æ¡ng Hiá»‡u Xuáº¥t Sáº¯c ChÃ¢u Ã 2022 & 2023Huy ChÆ°Æ¡ng VÃ ng Sáº£n Pháº©m, Dá»‹ch Vá»¥ Cháº¥t LÆ°á»£ng ChÃ¢u Ã 2023ThÆ°Æ¡ng Hiá»‡u Tháº©m Má»¹ Dáº«n Äáº§u Viá»‡t Nam 2024SEOUL CENTER - PHá»¤NG Sá»° Tá»ª TÃ‚M#SeoulCenter #ThamMyVien"""
    
    results = predictor.predict_text(test_text)
    print(predictor.format_results(results))


if __name__ == "__main__":
    main()