#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple Vietnamese NER Prediction
Chá»‰ cÃ³ function cÆ¡ báº£n Ä‘á»ƒ dá»± Ä‘oÃ¡n NER tá»« model Ä‘Ã£ train
"""

import torch
import os
import re
from transformers import AutoTokenizer, AutoModelForTokenClassification
from typing import List, Dict
from underthesea import sent_tokenize


class VietnameseNERPredictor:
    """Simple Vietnamese NER Predictor"""
    
    def __init__(self, model_path: str = "visobert_model/final"):
        """Initialize predictor with model"""
        self.model_path = os.path.join(os.path.dirname(__file__), model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)
        self.model.eval()

        self._TOKENIZER = self.tokenizer # alias
        self.MAX_LENGTH = 512
        self.tokenizer.model_max_length = self.MAX_LENGTH
        
        # Move to GPU if available
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print("Model loaded on GPU")
        else:
            print("Model loaded on CPU")

    def smart_chunk_text(self, item: dict, max_length: int = 510) -> List[dict]:
        """
        Chia nhá» vÄƒn báº£n thÃ nh cÃ¡c chunk dá»±a trÃªn sá»‘ lÆ°á»£ng TOKEN cá»§a model Visobert.
        max_length máº·c Ä‘á»‹nh lÃ  510 (Ä‘á»ƒ chá»«a chá»— cho 2 special tokens Ä‘áº§u vÃ  cuá»‘i).
        """
        text = item['text']
        original_labels = item.get('label', [])
        
        # TÃ¡ch cÃ¢u
        sentences = sent_tokenize(text)
        
        chunks = []
        
        # Biáº¿n lÆ°u trá»¯ cÃ¡c cÃ¢u trong chunk hiá»‡n táº¡i
        current_chunk_sentences = [] # List cÃ¡c tuple (start, end, text)
        current_token_count = 0      # Äá»•i tÃªn biáº¿n Ä‘á»ƒ rÃµ nghÄ©a hÆ¡n
        
        # Con trá» tÃ¬m kiáº¿m trong vÄƒn báº£n gá»‘c
        search_cursor = 0
        
        def flush_chunk(sent_buffer):
            if not sent_buffer:
                return
                
            chunk_start = sent_buffer[0][0]
            chunk_end = sent_buffer[-1][1]
            
            chunk_text = text[chunk_start:chunk_end]
            
            new_labels = []
            for lbl in original_labels:
                if lbl['start'] >= chunk_start and lbl['end'] <= chunk_end:
                    new_lbl = lbl.copy()
                    new_lbl['start'] = lbl['start'] - chunk_start
                    new_lbl['end'] = lbl['end'] - chunk_start
                    new_labels.append(new_lbl)
            
            chunks.append({
                "text": chunk_text, 
                "label": new_labels
            })

        for sent in sentences:
            # TÃ¬m vá»‹ trÃ­ chÃ­nh xÃ¡c cá»§a cÃ¢u
            sent_start = text.find(sent, search_cursor)
            if sent_start == -1: 
                sent_start = search_cursor
                
            sent_end = sent_start + len(sent)
            
            sent_token_ids = self.tokenizer.encode(sent, add_special_tokens=False)
            sent_token_count = len(sent_token_ids)
            
            # Kiá»ƒm tra giá»›i háº¡n token
            if current_token_count + sent_token_count <= max_length:
                current_chunk_sentences.append((sent_start, sent_end, sent))
                current_token_count += sent_token_count
            else:
                # ÄÃ³ng gÃ³i chunk cÅ©
                flush_chunk(current_chunk_sentences)
                
                # Táº¡o chunk má»›i
                current_chunk_sentences = [(sent_start, sent_end, sent)]
                current_token_count = sent_token_count
                
            # Cáº­p nháº­t con trá» tÃ¬m kiáº¿m
            search_cursor = sent_end

        # ÄÃ³ng gÃ³i chunk cuá»‘i cÃ¹ng
        if current_chunk_sentences:
            flush_chunk(current_chunk_sentences)
                
        return chunks

    def fix_bio_tags(self, tags):
        """
        Fix invalid BIO tag sequences by converting first I- tags without preceding B- to B- tags
        """
        fixed_tags = list(tags)
        
        for i in range(len(fixed_tags)):
            if fixed_tags[i].startswith('I-'):
                entity_type = fixed_tags[i][2:]
                
                if i == 0 or (not fixed_tags[i-1].startswith('B-' + entity_type) and 
                            not fixed_tags[i-1].startswith('I-' + entity_type)):
                    fixed_tags[i] = 'B-' + entity_type
        
        return fixed_tags

    def predict_text(self, text: str) -> Dict:
        """
        Dá»± Ä‘oÃ¡n entities trong vÄƒn báº£n
        
        Args:
            text: VÄƒn báº£n cáº§n dá»± Ä‘oÃ¡n
        """
        chunks = self.smart_chunk_text({'text': text}, max_length=self.MAX_LENGTH)
        all_entities = []
        current_offset = 0
        for chunk in chunks:
            chunk_text = chunk["text"]
            chunk_entities = self._predict_chunk(chunk_text)
            
            # Find the chunk in the original text starting from current_offset
            chunk_start = text.find(chunk_text, current_offset)
            if chunk_start != -1:
                for entity in chunk_entities:
                    entity["start"] = int(entity["start"] + chunk_start)
                    entity["end"] = int(entity["end"] + chunk_start)
                    all_entities.append(entity)
                
                # Update current_offset to after this chunk to avoid matching earlier occurrences
                current_offset = chunk_start + len(chunk_text)
            else:
                # If not found, skip adjusting offsets for this chunk
                print("Warning: chunk not found in original text while reconstructing offsets")
        
        entities_by_type = {}
        for entity in all_entities:
            label = entity["label"]
            if label not in entities_by_type:
                entities_by_type[label] = []
            entities_by_type[label].append(entity)
        
        return {
            "text": text,
            "entities": all_entities,
            "entities_by_type": entities_by_type,
            "total_entities": len(all_entities),
            "entity_types": list(entities_by_type.keys())
        }

    def _predict_chunk(self, text: str) -> List[Dict]:
        # Tokenize
        inputs = self.tokenizer(
            text, 
            padding="max_length",
            truncation=True,
            max_length=self.MAX_LENGTH,
            return_offsets_mapping=True,
            return_tensors="pt",
        )
        offset_mapping = inputs.pop("offset_mapping")
        
        # Move to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = outputs.logits.argmax(dim=-1)
            confidences = torch.softmax(outputs.logits, dim=-1).max(dim=-1)[0]
        
        # Convert to CPU
        predictions = predictions.cpu().numpy()[0]
        confidences = confidences.cpu().numpy()[0]
        offset_mapping = offset_mapping.cpu().numpy()[0]
        
        # Convert predictions to labels
        raw_labels = []
        valid_indices = []
        
        for i, (pred_id, conf, (start, end)) in enumerate(zip(predictions, confidences, offset_mapping)):
            if start == end:  # Skip special tokens
                continue
            
            label = self.model.config.id2label[int(pred_id)]
            raw_labels.append(label)
            valid_indices.append((i, conf, start, end))
        
        # Fix BIO tag sequences
        fixed_labels = self.fix_bio_tags(raw_labels)
        
        # Extract entities from fixed labels
        entities = []
        current_entity = None
        
        for (i, conf, start, end), label in zip(valid_indices, fixed_labels):
            if label == "O":
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
                continue
            
            entity_type = label[2:]  # Remove B- or I-
            
            if label.startswith("B-"):
                if current_entity:
                    entities.append(current_entity)
                
                current_entity = {
                    "text": text[start:end],
                    "label": entity_type,
                    "start": int(start),
                    "end": int(end),
                    "confidence": float(conf)
                }
            elif label.startswith("I-") and current_entity and current_entity["label"] == entity_type:
                current_entity["text"] += text[start:end]
                current_entity["end"] = int(end)
                current_entity["confidence"] = max(current_entity["confidence"], float(conf))
        
        if current_entity:
            entities.append(current_entity)
        
        return entities

    def format_results(self, results: Dict, show_confidence: bool = True) -> str:
        """Format káº¿t quáº£ Ä‘á»ƒ hiá»ƒn thá»‹"""
        output = []
        output.append(f"ğŸ“ Text: {results['text'][:100]}{'...' if len(results['text']) > 100 else ''}")
        output.append(f"ğŸ“Š Found {results['total_entities']} entities")
        output.append("")
        
        if results['entities']:
            output.append("ğŸ·ï¸ Detected Entities:")
            for i, entity in enumerate(results['entities'], 1):
                confidence_str = f" (confidence: {entity['confidence']:.3f})" if show_confidence else ""
                output.append(f"   {i}. '{entity['text']}' â†’ {entity['label']}{confidence_str}")
            
            output.append("")
            output.append("ğŸ“‹ By Category:")
            for entity_type in sorted(results['entities_by_type'].keys()):
                entities = results['entities_by_type'][entity_type]
                output.append(f"   {entity_type}: {len(entities)} entities")
                for entity in entities:
                    output.append(f"      â€¢ {entity['text']}")
        else:
            output.append("âŒ No entities detected")
        
        return "\n".join(output)


def main():
    """Test function Ä‘Æ¡n giáº£n"""
    # Simple test
    predictor = VietnameseNERPredictor()
    
    test_text ="""[MEGA LIVE 14.11] DEAL Äá»ˆNH THÃŒNH LÃŒNH - LÃ€M Äáº¸P SIÃŠU DÃNH CÃ¹ng Ca sÄ© THU THá»¦Y SÄƒn deal Äá»’NG GIÃ tá»« 99K - QuÃ  táº·ng LÃ m Ä‘áº¹p siÃªu khá»§ng----------Danh sÃ¡ch cÃ¡c dá»‹ch vá»¥ siÃªu Hot sáº½ xuáº¥t hiá»‡n trong phiÃªn live láº§n nÃ y, vá»›i má»©c giÃ¡ Äá»˜C QUYá»€N siÃªu giáº£m:- ChÄƒm SÃ³c Da Cao Cáº¥p 3in1- Dr.Vip ChÄƒm SÃ³c Da LÃ£o HoÃ¡ ECM- Dr.Vip á»¦ Tráº¯ng Face Collagen- Dr.Vip ChÄƒm SÃ³c VÃ¹ng Máº¯t ECM - XoÃ¡ nhÄƒn váº¿t chÃ¢n chim- Dr.Vip Collagen Thuá»· PhÃ¢n - á»¨c Cháº¿ Äá»‘m NÃ¢u- Dr. Acne Trá»‹ Má»¥n Chuáº©n Y Khoa- Dr.Seoul Laser Pico 5.0- Dr.Slim Giáº£m Má»¡ Exilis Detox- Dr. White Táº¯m Tráº¯ng HoÃ ng Gia- Phun mÃ y- Phun mÃ­- Phun mÃ´iNgoÃ i ra, cÃ¡c hoáº¡t Ä‘á»™ng cá»™ng hÆ°á»Ÿng táº¡i phiÃªn live: Giao lÆ°u, trÃ² chuyá»‡n, chia sáº» kiáº¿n thá»©c lÃ m Ä‘áº¹p cÃ¹ng ca sÄ© Thu Thá»§y TÆ° váº¥n & giáº£i Ä‘Ã¡p vá» dá»‹ch vá»¥ cÃ¹ng Seoul Center Tham gia minigame - Nháº­n quÃ  Ä‘á»™c quyá»n thÆ°Æ¡ng hiá»‡uTáº¥t cáº£ DEAL há»i Ä‘Ã£ sáºµn sÃ ng "lÃªn ká»‡" vÃ o lÃºc 19h00 | 14.11.2024 táº¡i FB/ Tiktok Seoul Center vÃ  Fb/tiktok ca sÄ© Thu Thá»§y Giáº£m giÃ¡ ká»‹ch sÃ n, chá»‰ cÃ³ trÃªn live Äáº·t lá»‹ch sÄƒn ngay lÃ m Ä‘áº¹p Ä‘Ã³n táº¿t cÃ¹ng Thu Thá»§y nhÃ©!-------------Há»‡ Thá»‘ng Tháº©m Má»¹ Quá»‘c Táº¿ Seoul CenterSáºµn sÃ ng láº¯ng nghe má»i Ã½ kiáº¿n cá»§a khÃ¡ch hÃ ng: 1800 3333Äáº·t lá»‹ch ngay vá»›i Top dá»‹ch vá»¥ Ä‘áº·c quyá»n: Website: Zalo: Tiktok: Youtube: Top 10 ThÆ°Æ¡ng Hiá»‡u Xuáº¥t Sáº¯c ChÃ¢u Ã 2022 & 2023Huy ChÆ°Æ¡ng VÃ ng Sáº£n Pháº©m, Dá»‹ch Vá»¥ Cháº¥t LÆ°á»£ng ChÃ¢u Ã 2023ThÆ°Æ¡ng Hiá»‡u Tháº©m Má»¹ Dáº«n Äáº§u Viá»‡t Nam 2024SEOUL CENTER - PHá»¤NG Sá»° Tá»ª TÃ‚M#SeoulCenter #ThamMyVien"""
    
    results = predictor.predict_text(test_text)
    print(predictor.format_results(results))


if __name__ == "__main__":
    main()